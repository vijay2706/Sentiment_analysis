{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b77fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86704465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.X_train= None   #train data before splitting validation data\n",
    "        self.X_test= None    #test data\n",
    "        self.y_train= None   #train data before splitting validation data\n",
    "        self.y_test= None    #test data\n",
    "        self.X_train_seq= None  #train data converted to sequences\n",
    "        self.X_test_seq= None   #test data converted to sequences\n",
    "        \n",
    "        self.y_train_le= None   #y_train after applying fit_transform\n",
    "        self.y_test_le= None    #y_test after applying transform\n",
    "        self.y_train_oh = None  #y_train after one hot enconding\n",
    "        self.y_test_oh = None   #y_test after one hot encoding\n",
    "        \n",
    "        self.X_train_rest= None  #X_train after splitting validation data\n",
    "        self.X_valid= None       #X_valid - validation data\n",
    "        self.y_train_rest= None  #y_train after splitting validation data\n",
    "        self.y_valid= None       #y_valid - validation data\n",
    "        \n",
    "        self.token= None         \n",
    "        \n",
    "        self.epoch_stop= 4      #no. of epochs after which model starts overfitting\n",
    "        \n",
    "        self.NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "        self.VAL_SIZE = 1000  # Size of the validation set\n",
    "        self.NB_START_EPOCHS = 20  # Number of epochs for training\n",
    "        self.BATCH_SIZE = 512  # Size of the batches\n",
    "        \n",
    "        self.base_model= None   \n",
    "        self.reduced_model= None\n",
    "        self.reg_model= None\n",
    "        self.drop_model= None\n",
    "        \n",
    "        self.main_model= None\n",
    "        \n",
    "        self.history= None\n",
    "        \n",
    "        \n",
    "    def remove_stopwords(self,input_text):\n",
    "        '''function to remove some stopwords, which are not useful for analysis'''\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def remove_mentions(self,input_text):\n",
    "        '''function to remove @ from every text'''\n",
    "        return re.sub(r'@\\w+', '', input_text)    \n",
    "    \n",
    "    def one_hot_seq(self,seqs):\n",
    "        '''function to convert sequences into one hot encoding'''\n",
    "        ohs = np.zeros((len(seqs), self.NB_WORDS))\n",
    "        for i, s in enumerate(seqs):\n",
    "            ohs[i, s] = 1.\n",
    "        return ohs\n",
    "        \n",
    "    def tokenize(self):\n",
    "        \n",
    "        self.token = Tokenizer(num_words=self.NB_WORDS,\n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "        self.token.fit_on_texts(self.X_train)\n",
    "    \n",
    "    def tokennize_to_seq(self):\n",
    "        '''converts texts to sequrnces'''\n",
    "        self.X_train_seq = self.token.texts_to_sequences(self.X_train)\n",
    "        self.X_test_seq = self.token.texts_to_sequences(self.X_test)\n",
    "    \n",
    "    def encoders(self):\n",
    "        le = LabelEncoder()\n",
    "        self.y_train_le = le.fit_transform(self.y_train)\n",
    "        self.y_test_le = le.transform(self.y_test)\n",
    "        self.y_train_oh = to_categorical(self.y_train_le)\n",
    "        self.y_test_oh = to_categorical(self.y_test_le)\n",
    "    \n",
    "    def get_valid_data(self):\n",
    "        self.X_train_rest, self.X_valid, self.y_train_rest, self.y_valid = train_test_split(self.X_train_oh, self.y_train_oh, test_size=0.1, random_state=1)\n",
    "\n",
    "    def base_model_fun(self):\n",
    "        '''base model'''\n",
    "        self.base_model = models.Sequential()\n",
    "        self.base_model.add(layers.Dense(64, activation='relu', input_shape=(self.NB_WORDS,)))\n",
    "        self.base_model.add(layers.Dense(64, activation='relu'))\n",
    "        self.base_model.add(layers.Dense(2, activation='softmax'))\n",
    "        \n",
    "        \n",
    "    def reduced_model_fun(self):\n",
    "        '''reducing the complexity of the base model'''\n",
    "        self.reduced_model = models.Sequential()\n",
    "        self.reduced_model.add(layers.Dense(32, activation='relu', input_shape=(self.NB_WORDS,)))\n",
    "        self.reduced_model.add(layers.Dense(2, activation='softmax'))\n",
    "        \n",
    "    def reg_model_fun(self):\n",
    "        '''adding regularization in the base model'''\n",
    "        self.reg_model = models.Sequential()\n",
    "        self.reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(self.NB_WORDS,)))\n",
    "        self.reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "        self.reg_model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    def drop_model_fun(self):\n",
    "        '''adding drop out layers in the base model'''\n",
    "        self.drop_model = models.Sequential()\n",
    "        self.drop_model.add(layers.Dense(64, activation='relu', input_shape=(self.NB_WORDS,)))\n",
    "        self.drop_model.add(layers.Dropout(0.5))\n",
    "        self.drop_model.add(layers.Dense(64, activation='relu'))\n",
    "        self.drop_model.add(layers.Dropout(0.5))\n",
    "        self.drop_model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    def deep_model(self,model):\n",
    "        '''compile and train for 20 epochs'''\n",
    "        model.compile(optimizer='adam'\n",
    "                      , loss='categorical_crossentropy'\n",
    "                      , metrics=['accuracy'])\n",
    "\n",
    "        self.history = model.fit(self.X_train_rest\n",
    "                           , self.y_train_rest\n",
    "                           , epochs=self.NB_START_EPOCHS\n",
    "                           , batch_size=self.BATCH_SIZE\n",
    "                           , validation_data=(self.X_valid, self.y_valid)\n",
    "                           , verbose=0)\n",
    "\n",
    "    def eval_metric(self, metric_name):\n",
    "        metric = self.history.history[metric_name]\n",
    "        val_metric = self.history.history['val_' + metric_name]\n",
    "\n",
    "        e = range(1, self.NB_START_EPOCHS + 1)\n",
    "\n",
    "        plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "        plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def test_model(self,model):\n",
    "        '''Since, the models are overfitting, training for epochs upto which the loss graph is not increasing'''\n",
    "        model.compile(optimizer='adam'\n",
    "                      , loss='categorical_crossentropy'\n",
    "                      , metrics=['accuracy'])\n",
    "        model.fit(self.X_train_oh\n",
    "                  , self.y_train_oh\n",
    "                  , epochs=self.epoch_stop\n",
    "                  , batch_size=self.BATCH_SIZE\n",
    "                  , verbose=0)\n",
    "        results = model.evaluate(self.X_test_oh, self.y_test_oh)\n",
    "\n",
    "        #print(results)\n",
    "        return results\n",
    "    \n",
    "    def predict_class(self,text):\n",
    "        '''Function to predict sentiment class of the passed text'''\n",
    "\n",
    "        sentiment_classes = ['Negative', 'Positive']\n",
    "\n",
    "        # Transforms text to a sequence of integers using a tokenizer object\n",
    "        xt = self.token.texts_to_sequences(text)\n",
    "        ohs = np.zeros((1,10000))\n",
    "        for i in xt[0] :\n",
    "            ohs[0][i]= 1\n",
    "        yt = self.drop_model.predict(ohs).argmax()\n",
    "        print('The predicted sentiment is', sentiment_classes[yt])\n",
    "        return sentiment_classes[yt]\n",
    "    \n",
    "    def read_data (self):\n",
    "        '''function to read data and split into train and test'''\n",
    "        data= pd.read_csv(\"airline_sentiment_analysis.csv\")\n",
    "        data = data[['text', 'airline_sentiment']]\n",
    "        data.text = data.text.apply(self.remove_stopwords).apply(self.remove_mentions)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(data.text, data.airline_sentiment, test_size=0.1, random_state=1)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        '''the pipeline to get train model'''\n",
    "        self.read_data()\n",
    "        self.tokenize()\n",
    "        self.tokennize_to_seq()\n",
    "        self.X_train_oh = self.one_hot_seq(self.X_train_seq)\n",
    "        self.X_test_oh = self.one_hot_seq(self.X_test_seq) \n",
    "        self.encoders()\n",
    "        self.get_valid_data()\n",
    "        self.drop_model_fun()\n",
    "        self.main_model= self.drop_model\n",
    "        #self.deep_model(self.main_model)\n",
    "        self.test_model(self.main_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0409bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9264\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model = Model()\n",
    "model.train()\n",
    "   \n",
    "# Defining path operation for root endpoint\n",
    "@app.get('/')\n",
    "def main():\n",
    "    return {'message': 'Welcome!'}\n",
    "  \n",
    "# Defining path operation for /name endpoint\n",
    "@app.post('/predict')\n",
    "def predict(data : str):\n",
    "    # Making the data in a form suitable for prediction\n",
    "    # Predicting the Class\n",
    "     \n",
    "      \n",
    "    # Return the Result\n",
    "    return model.predict_class([data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
